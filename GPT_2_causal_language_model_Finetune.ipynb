{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14tAt4EDU5lo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfDByDEsU9-y"
      },
      "outputs": [],
      "source": [
        "questions_answers = [\n",
        "    (\"What is the color of the sky?\", \"Blue\"),\n",
        "    (\"What do bees make?\", \"Honey\"),\n",
        "        (\"What is the largest planet?\", \"Jupiter\"),\n",
        "    (\"Who wrote Hamlet?\", \"Shakespeare\"),\n",
        "    (\"What liquid do cars need?\", \"Fuel\"),\n",
        "    (\"What is frozen water called?\", \"Ice\"),\n",
        "    (\"Which animal is known as man's best friend?\", \"Dog\"),\n",
        "    (\"What do we breathe?\", \"Air\"),\n",
        "    (\"What color is a ruby?\", \"Red\"),\n",
        "    (\"What do bees produce?\", \"Honey\"),\n",
        "    (\"What is the opposite of cold?\", \"Hot\"),\n",
        "    (\"What do we call a baby cat?\", \"Kitten\"),\n",
        "    (\"What do you use to write on a blackboard?\", \"Chalk\"),\n",
        "    (\"What is the capital of France?\", \"Paris\"),\n",
        "    (\"What fruit is known for its potassium?\", \"Banana\"),\n",
        "    (\"What is the hardest natural substance?\", \"Diamond\"),\n",
        "    (\"What season follows summer?\", \"Autumn\"),\n",
        "    (\"What is the currency of the USA?\", \"Dollar\"),\n",
        "    (\"What is the primary language in Spain?\", \"Spanish\"),\n",
        "    (\"What is the color of grass?\", \"Green\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JD1I_JJU-Bx"
      },
      "outputs": [],
      "source": [
        "class QADataset(Dataset):\n",
        "    def __init__(self, tokenizer, qa_list, max_length):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.qa_list = qa_list\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question, answer = self.qa_list[idx]\n",
        "        encodings = self.tokenizer(f\"{question} {answer}\", truncation=True, max_length=self.max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        return encodings.input_ids[0], encodings.attention_mask[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "26bd0dcfbb2d468eb4b0252416d013d7",
            "e9831e6ea3544d2bbbd8d8798610f58c",
            "70a4b93fade44838915a02c8d272ab58",
            "4c1a03ff121f4ef3a3774f2c7069a086",
            "dac81218cee043428a5062f12bc719a4",
            "005ee0edfb0b4cfca26fd9769ba97b7b",
            "229be416662044a1b9619e7a4cab1041",
            "f3df2ac6a78d4661a1db8863def685a4",
            "d685128ee8204ff08615c5211c843754",
            "0d0e447e199341ba88ad579a2ada22b8",
            "39457de2528742d797ae4f5cd7befe42",
            "b829048391fd43189aa8a449f4f86778",
            "159e172528a84f9abd44bde3ad316d4f",
            "61bd742ad23d40aab70b8215d83dc064",
            "410c5af14713485286af1d8596b6062c",
            "7941b5618d48477d95d51f3d0d4a10b0",
            "58f4069f6e1b4b7a8bf98976df672479",
            "154dc83768ee49428bc2138f072cce8a",
            "ef0fabc5b01547da8c4dccbc8354dbe4",
            "a34d6c8955d748399f3ce99a2a422fb7",
            "0b6fcd09dcfa40b8848c0280e2b9ffde",
            "175bb9d7dc1e4803be98520756e643a9",
            "8d63ba2341a544cc837f6f88715dd864",
            "e978e3ce4db64cedb5be406da51c8fcc",
            "6b7cb13fd531438f8dc53668214f0e88",
            "802a06650305403f9b3be7cb4a5d6e2b",
            "7176a69714e145e4a9bf31af24b9b1ae",
            "0e66823fdc0742e886b203cf04053959",
            "4280a2de969f478c9cc296df349e9937",
            "63bdb4d2bc864d7ea5736985be15ae44",
            "2a308f2daf6646cbaadbd527bafcf06c",
            "8d1bbdef67ce49e98ed6862583088420",
            "865c7cee17ed4d719c22f2850b9d43cb",
            "7c1ec0dec6184a18a5a31dde25e5720f",
            "dc88471c4bd846d4b11d2a768b04ad1f",
            "cae827c095ef4791b0123da1e4d59c2b",
            "27bfa2744ce24999ac6a5e19299739fe",
            "0fc98faba6a6402a87c6206f71101594",
            "9ebb7a8539ee4295bf4ab07ef1b1baf8",
            "317a205611f943f4969e8aec343e38d1",
            "004750520a864dd9a08e173d813f5c8f",
            "35a16c208f254f2d85190d1cb223241a",
            "5781d440c1354a718a76151769ecfce9",
            "b7aba28d487c482ea4079451423f5a49",
            "8b02217a675444c2a08f0f99b887d72b",
            "f71bb8300f6a4559a77d8e99e11ccb57",
            "2c8a2424f6e94e68a8d479d5e705456a",
            "7f992b02ef9542bca0fdedad4d0ea75d",
            "92cc4458d7604a2b85944b3bbf9fb1d8",
            "955742c69f604fd587183c8bd29cfaf0",
            "ed975fcdd72747b78e8d938b4eb7275d",
            "88c18d8a2d7a4db2acc7ec7b160115e0",
            "9d70defffb4a4fb6b1516f3e73cba603",
            "e03a08d8964e46d78a7d61cef57ef527",
            "e9dd036c8e8d4997b37b323a4c7b9d41",
            "6de61fac7b1a46eebaa17b48a5bc5830",
            "0552ab89169c46a6bb8d08d027ad71eb",
            "80e6b026fa86430b90066dcbcde0bb12",
            "9b60dc82d98d4a76882251c5f5318849",
            "bc169bdc440b4b1488ba44d9b1730058",
            "28c04f5df90f4474b00eb135f30213f7",
            "4824a7a562ea4255b73f25e332ae8548",
            "9d00693504494fee8b658f4b42b90e8f",
            "4e6571639c6342c9b02b5fc1622c741a",
            "47dd7cdd6750419b8170af194393dea8",
            "ae4cd971b27f4f14874eb3f4f9363d8d"
          ]
        },
        "id": "VqPY8ksDU-Ez",
        "outputId": "fe481abb-e4ba-48d7-ba51-f707ab5c5468"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul0g9HpPU-Hx"
      },
      "outputs": [],
      "source": [
        "max_length = 32 # Define the maximum length for the sequences\n",
        "dataset = QADataset(tokenizer, questions_answers, max_length)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEvs7sW-VPcb",
        "outputId": "a1596476-952b-49ea-bca8-a8fa1717ee0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCXlC58_VPfP",
        "outputId": "fd102e3a-cc79-4269-9f53-772bc7d0b50a"
      },
      "outputs": [],
      "source": [
        "num_epochs = 48\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in dataloader:\n",
        "        inputs, masks = batch\n",
        "        inputs, masks = inputs.to(device), masks.to(device)\n",
        "\n",
        "        outputs = model(inputs, labels=inputs, attention_mask=masks)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjbd5QB6VPh7",
        "outputId": "4537dd18-7fbe-45e8-befc-eb8f0e081aad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what is the captial city of America? Chicago\n"
          ]
        }
      ],
      "source": [
        "def generate_answer(question, model, tokenizer, max_length=50):\n",
        "    # Tokenize the input question\n",
        "    input_ids = tokenizer.encode(question, return_tensors='pt').to(device)\n",
        "\n",
        "    # Generate the output (answer) using the model\n",
        "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # Decode the output to a human-readable format\n",
        "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "question = \"what is the captial city of America?\"\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "answer = generate_answer(question, model, tokenizer)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1HPXoxdU-cm",
        "outputId": "85353053-bdf5-4b29-ab1e-fc077525fff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what is the color of blood? Red\n"
          ]
        }
      ],
      "source": [
        "question = \"what is the color of blood?\"\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "answer = generate_answer(question, model, tokenizer)\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
